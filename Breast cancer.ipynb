{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00451/dataR2.csv\", delimiter=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>HOMA</th>\n",
       "      <th>Leptin</th>\n",
       "      <th>Adiponectin</th>\n",
       "      <th>Resistin</th>\n",
       "      <th>MCP.1</th>\n",
       "      <th>Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>70</td>\n",
       "      <td>2.707</td>\n",
       "      <td>0.467409</td>\n",
       "      <td>8.8071</td>\n",
       "      <td>9.702400</td>\n",
       "      <td>7.99585</td>\n",
       "      <td>417.114</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83</td>\n",
       "      <td>20.690495</td>\n",
       "      <td>92</td>\n",
       "      <td>3.115</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>8.8438</td>\n",
       "      <td>5.429285</td>\n",
       "      <td>4.06405</td>\n",
       "      <td>468.786</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>23.124670</td>\n",
       "      <td>91</td>\n",
       "      <td>4.498</td>\n",
       "      <td>1.009651</td>\n",
       "      <td>17.9393</td>\n",
       "      <td>22.432040</td>\n",
       "      <td>9.27715</td>\n",
       "      <td>554.697</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68</td>\n",
       "      <td>21.367521</td>\n",
       "      <td>77</td>\n",
       "      <td>3.226</td>\n",
       "      <td>0.612725</td>\n",
       "      <td>9.8827</td>\n",
       "      <td>7.169560</td>\n",
       "      <td>12.76600</td>\n",
       "      <td>928.220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86</td>\n",
       "      <td>21.111111</td>\n",
       "      <td>92</td>\n",
       "      <td>3.549</td>\n",
       "      <td>0.805386</td>\n",
       "      <td>6.6994</td>\n",
       "      <td>4.819240</td>\n",
       "      <td>10.57635</td>\n",
       "      <td>773.920</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age        BMI  Glucose  Insulin      HOMA   Leptin  Adiponectin  Resistin  \\\n",
       "0   48  23.500000       70    2.707  0.467409   8.8071     9.702400   7.99585   \n",
       "1   83  20.690495       92    3.115  0.706897   8.8438     5.429285   4.06405   \n",
       "2   82  23.124670       91    4.498  1.009651  17.9393    22.432040   9.27715   \n",
       "3   68  21.367521       77    3.226  0.612725   9.8827     7.169560  12.76600   \n",
       "4   86  21.111111       92    3.549  0.805386   6.6994     4.819240  10.57635   \n",
       "\n",
       "     MCP.1  Classification  \n",
       "0  417.114               1  \n",
       "1  468.786               1  \n",
       "2  554.697               1  \n",
       "3  928.220               1  \n",
       "4  773.920               1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, df.columns != 'Classification']\n",
    "y = df.loc[:, df.columns == 'Classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>HOMA</th>\n",
       "      <th>Leptin</th>\n",
       "      <th>Adiponectin</th>\n",
       "      <th>Resistin</th>\n",
       "      <th>MCP.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>70</td>\n",
       "      <td>2.707</td>\n",
       "      <td>0.467409</td>\n",
       "      <td>8.8071</td>\n",
       "      <td>9.702400</td>\n",
       "      <td>7.99585</td>\n",
       "      <td>417.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83</td>\n",
       "      <td>20.690495</td>\n",
       "      <td>92</td>\n",
       "      <td>3.115</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>8.8438</td>\n",
       "      <td>5.429285</td>\n",
       "      <td>4.06405</td>\n",
       "      <td>468.786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>23.124670</td>\n",
       "      <td>91</td>\n",
       "      <td>4.498</td>\n",
       "      <td>1.009651</td>\n",
       "      <td>17.9393</td>\n",
       "      <td>22.432040</td>\n",
       "      <td>9.27715</td>\n",
       "      <td>554.697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68</td>\n",
       "      <td>21.367521</td>\n",
       "      <td>77</td>\n",
       "      <td>3.226</td>\n",
       "      <td>0.612725</td>\n",
       "      <td>9.8827</td>\n",
       "      <td>7.169560</td>\n",
       "      <td>12.76600</td>\n",
       "      <td>928.220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86</td>\n",
       "      <td>21.111111</td>\n",
       "      <td>92</td>\n",
       "      <td>3.549</td>\n",
       "      <td>0.805386</td>\n",
       "      <td>6.6994</td>\n",
       "      <td>4.819240</td>\n",
       "      <td>10.57635</td>\n",
       "      <td>773.920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>49</td>\n",
       "      <td>22.854458</td>\n",
       "      <td>92</td>\n",
       "      <td>3.226</td>\n",
       "      <td>0.732087</td>\n",
       "      <td>6.8317</td>\n",
       "      <td>13.679750</td>\n",
       "      <td>10.31760</td>\n",
       "      <td>530.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>89</td>\n",
       "      <td>22.700000</td>\n",
       "      <td>77</td>\n",
       "      <td>4.690</td>\n",
       "      <td>0.890787</td>\n",
       "      <td>6.9640</td>\n",
       "      <td>5.589865</td>\n",
       "      <td>12.93610</td>\n",
       "      <td>1256.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>76</td>\n",
       "      <td>23.800000</td>\n",
       "      <td>118</td>\n",
       "      <td>6.470</td>\n",
       "      <td>1.883201</td>\n",
       "      <td>4.3110</td>\n",
       "      <td>13.251320</td>\n",
       "      <td>5.10420</td>\n",
       "      <td>280.694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>73</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>97</td>\n",
       "      <td>3.350</td>\n",
       "      <td>0.801543</td>\n",
       "      <td>4.4700</td>\n",
       "      <td>10.358725</td>\n",
       "      <td>6.28445</td>\n",
       "      <td>136.855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>75</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>83</td>\n",
       "      <td>4.952</td>\n",
       "      <td>1.013839</td>\n",
       "      <td>17.1270</td>\n",
       "      <td>11.578990</td>\n",
       "      <td>7.09130</td>\n",
       "      <td>318.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>34</td>\n",
       "      <td>21.470000</td>\n",
       "      <td>78</td>\n",
       "      <td>3.469</td>\n",
       "      <td>0.667436</td>\n",
       "      <td>14.5700</td>\n",
       "      <td>13.110000</td>\n",
       "      <td>6.92000</td>\n",
       "      <td>354.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>29</td>\n",
       "      <td>23.010000</td>\n",
       "      <td>82</td>\n",
       "      <td>5.663</td>\n",
       "      <td>1.145436</td>\n",
       "      <td>35.5900</td>\n",
       "      <td>26.720000</td>\n",
       "      <td>4.58000</td>\n",
       "      <td>174.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25</td>\n",
       "      <td>22.860000</td>\n",
       "      <td>82</td>\n",
       "      <td>4.090</td>\n",
       "      <td>0.827271</td>\n",
       "      <td>20.4500</td>\n",
       "      <td>23.670000</td>\n",
       "      <td>5.14000</td>\n",
       "      <td>313.730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>24</td>\n",
       "      <td>18.670000</td>\n",
       "      <td>88</td>\n",
       "      <td>6.107</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>8.8800</td>\n",
       "      <td>36.060000</td>\n",
       "      <td>6.85000</td>\n",
       "      <td>632.220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>38</td>\n",
       "      <td>23.340000</td>\n",
       "      <td>75</td>\n",
       "      <td>5.782</td>\n",
       "      <td>1.069670</td>\n",
       "      <td>15.2600</td>\n",
       "      <td>17.950000</td>\n",
       "      <td>9.35000</td>\n",
       "      <td>165.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>44</td>\n",
       "      <td>20.760000</td>\n",
       "      <td>86</td>\n",
       "      <td>7.553</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>14.0900</td>\n",
       "      <td>20.320000</td>\n",
       "      <td>7.64000</td>\n",
       "      <td>63.610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>47</td>\n",
       "      <td>22.030000</td>\n",
       "      <td>84</td>\n",
       "      <td>2.869</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>26.6500</td>\n",
       "      <td>38.040000</td>\n",
       "      <td>3.32000</td>\n",
       "      <td>191.720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>61</td>\n",
       "      <td>32.038959</td>\n",
       "      <td>85</td>\n",
       "      <td>18.077</td>\n",
       "      <td>3.790144</td>\n",
       "      <td>30.7729</td>\n",
       "      <td>7.780255</td>\n",
       "      <td>13.68392</td>\n",
       "      <td>444.395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>64</td>\n",
       "      <td>34.529723</td>\n",
       "      <td>95</td>\n",
       "      <td>4.427</td>\n",
       "      <td>1.037394</td>\n",
       "      <td>21.2117</td>\n",
       "      <td>5.462620</td>\n",
       "      <td>6.70188</td>\n",
       "      <td>252.449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>32</td>\n",
       "      <td>36.512637</td>\n",
       "      <td>87</td>\n",
       "      <td>14.026</td>\n",
       "      <td>3.009980</td>\n",
       "      <td>49.3727</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>17.10223</td>\n",
       "      <td>588.460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>36</td>\n",
       "      <td>28.576676</td>\n",
       "      <td>86</td>\n",
       "      <td>4.345</td>\n",
       "      <td>0.921719</td>\n",
       "      <td>15.1248</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>9.15390</td>\n",
       "      <td>534.224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>34</td>\n",
       "      <td>31.975015</td>\n",
       "      <td>87</td>\n",
       "      <td>4.530</td>\n",
       "      <td>0.972138</td>\n",
       "      <td>28.7502</td>\n",
       "      <td>7.642760</td>\n",
       "      <td>5.62592</td>\n",
       "      <td>572.783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>29</td>\n",
       "      <td>32.270788</td>\n",
       "      <td>84</td>\n",
       "      <td>5.810</td>\n",
       "      <td>1.203832</td>\n",
       "      <td>45.6196</td>\n",
       "      <td>6.209635</td>\n",
       "      <td>24.60330</td>\n",
       "      <td>904.981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>35</td>\n",
       "      <td>30.276817</td>\n",
       "      <td>84</td>\n",
       "      <td>4.376</td>\n",
       "      <td>0.906707</td>\n",
       "      <td>39.2134</td>\n",
       "      <td>9.048185</td>\n",
       "      <td>16.43706</td>\n",
       "      <td>733.797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>54</td>\n",
       "      <td>30.483158</td>\n",
       "      <td>90</td>\n",
       "      <td>5.537</td>\n",
       "      <td>1.229214</td>\n",
       "      <td>12.3310</td>\n",
       "      <td>9.731380</td>\n",
       "      <td>10.19299</td>\n",
       "      <td>1227.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>45</td>\n",
       "      <td>37.035608</td>\n",
       "      <td>83</td>\n",
       "      <td>6.760</td>\n",
       "      <td>1.383997</td>\n",
       "      <td>39.9802</td>\n",
       "      <td>4.617125</td>\n",
       "      <td>8.70448</td>\n",
       "      <td>586.173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>50</td>\n",
       "      <td>38.578759</td>\n",
       "      <td>106</td>\n",
       "      <td>6.703</td>\n",
       "      <td>1.752611</td>\n",
       "      <td>46.6401</td>\n",
       "      <td>4.667645</td>\n",
       "      <td>11.78388</td>\n",
       "      <td>887.160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>66</td>\n",
       "      <td>31.446541</td>\n",
       "      <td>90</td>\n",
       "      <td>9.245</td>\n",
       "      <td>2.052390</td>\n",
       "      <td>45.9624</td>\n",
       "      <td>10.355260</td>\n",
       "      <td>23.38190</td>\n",
       "      <td>1102.110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>35</td>\n",
       "      <td>35.250761</td>\n",
       "      <td>90</td>\n",
       "      <td>6.817</td>\n",
       "      <td>1.513374</td>\n",
       "      <td>50.6094</td>\n",
       "      <td>6.966895</td>\n",
       "      <td>22.03703</td>\n",
       "      <td>667.928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>36</td>\n",
       "      <td>34.174890</td>\n",
       "      <td>80</td>\n",
       "      <td>6.590</td>\n",
       "      <td>1.300427</td>\n",
       "      <td>10.2809</td>\n",
       "      <td>5.065915</td>\n",
       "      <td>15.72187</td>\n",
       "      <td>581.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>48</td>\n",
       "      <td>28.125000</td>\n",
       "      <td>90</td>\n",
       "      <td>2.540</td>\n",
       "      <td>0.563880</td>\n",
       "      <td>15.5325</td>\n",
       "      <td>10.222310</td>\n",
       "      <td>16.11032</td>\n",
       "      <td>1698.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>85</td>\n",
       "      <td>27.688778</td>\n",
       "      <td>196</td>\n",
       "      <td>51.814</td>\n",
       "      <td>25.050342</td>\n",
       "      <td>70.8824</td>\n",
       "      <td>7.901685</td>\n",
       "      <td>55.21530</td>\n",
       "      <td>1078.359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>48</td>\n",
       "      <td>31.250000</td>\n",
       "      <td>199</td>\n",
       "      <td>12.162</td>\n",
       "      <td>5.969920</td>\n",
       "      <td>18.1314</td>\n",
       "      <td>4.104105</td>\n",
       "      <td>53.63080</td>\n",
       "      <td>1698.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>58</td>\n",
       "      <td>29.154519</td>\n",
       "      <td>139</td>\n",
       "      <td>16.582</td>\n",
       "      <td>5.685415</td>\n",
       "      <td>22.8884</td>\n",
       "      <td>10.262660</td>\n",
       "      <td>13.97399</td>\n",
       "      <td>923.886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>40</td>\n",
       "      <td>30.836531</td>\n",
       "      <td>128</td>\n",
       "      <td>41.894</td>\n",
       "      <td>13.227332</td>\n",
       "      <td>31.0385</td>\n",
       "      <td>6.160995</td>\n",
       "      <td>17.55503</td>\n",
       "      <td>638.261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>82</td>\n",
       "      <td>31.217482</td>\n",
       "      <td>100</td>\n",
       "      <td>18.077</td>\n",
       "      <td>4.458993</td>\n",
       "      <td>31.6453</td>\n",
       "      <td>9.923650</td>\n",
       "      <td>19.94687</td>\n",
       "      <td>994.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>52</td>\n",
       "      <td>30.801249</td>\n",
       "      <td>87</td>\n",
       "      <td>30.212</td>\n",
       "      <td>6.483495</td>\n",
       "      <td>29.2739</td>\n",
       "      <td>6.268540</td>\n",
       "      <td>24.24591</td>\n",
       "      <td>764.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>49</td>\n",
       "      <td>32.461911</td>\n",
       "      <td>134</td>\n",
       "      <td>24.887</td>\n",
       "      <td>8.225983</td>\n",
       "      <td>42.3914</td>\n",
       "      <td>10.793940</td>\n",
       "      <td>5.76800</td>\n",
       "      <td>656.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>60</td>\n",
       "      <td>31.231410</td>\n",
       "      <td>131</td>\n",
       "      <td>30.130</td>\n",
       "      <td>9.736007</td>\n",
       "      <td>37.8430</td>\n",
       "      <td>8.404430</td>\n",
       "      <td>11.50005</td>\n",
       "      <td>396.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>49</td>\n",
       "      <td>29.777778</td>\n",
       "      <td>70</td>\n",
       "      <td>8.396</td>\n",
       "      <td>1.449709</td>\n",
       "      <td>51.3387</td>\n",
       "      <td>10.731740</td>\n",
       "      <td>20.76801</td>\n",
       "      <td>602.486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>44</td>\n",
       "      <td>27.887617</td>\n",
       "      <td>99</td>\n",
       "      <td>9.208</td>\n",
       "      <td>2.248594</td>\n",
       "      <td>12.6757</td>\n",
       "      <td>5.478170</td>\n",
       "      <td>23.03306</td>\n",
       "      <td>407.206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>40</td>\n",
       "      <td>27.636054</td>\n",
       "      <td>103</td>\n",
       "      <td>2.432</td>\n",
       "      <td>0.617890</td>\n",
       "      <td>14.3224</td>\n",
       "      <td>6.783870</td>\n",
       "      <td>26.01360</td>\n",
       "      <td>293.123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>71</td>\n",
       "      <td>27.915519</td>\n",
       "      <td>104</td>\n",
       "      <td>18.200</td>\n",
       "      <td>4.668907</td>\n",
       "      <td>53.4997</td>\n",
       "      <td>1.656020</td>\n",
       "      <td>49.24184</td>\n",
       "      <td>256.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>69</td>\n",
       "      <td>28.444444</td>\n",
       "      <td>108</td>\n",
       "      <td>8.808</td>\n",
       "      <td>2.346451</td>\n",
       "      <td>14.7485</td>\n",
       "      <td>5.288025</td>\n",
       "      <td>16.48508</td>\n",
       "      <td>353.568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>74</td>\n",
       "      <td>28.650138</td>\n",
       "      <td>88</td>\n",
       "      <td>3.012</td>\n",
       "      <td>0.653805</td>\n",
       "      <td>31.1233</td>\n",
       "      <td>7.652220</td>\n",
       "      <td>18.35574</td>\n",
       "      <td>572.401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>66</td>\n",
       "      <td>26.562500</td>\n",
       "      <td>89</td>\n",
       "      <td>6.524</td>\n",
       "      <td>1.432235</td>\n",
       "      <td>14.9084</td>\n",
       "      <td>8.429960</td>\n",
       "      <td>14.91922</td>\n",
       "      <td>269.487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>65</td>\n",
       "      <td>30.915577</td>\n",
       "      <td>97</td>\n",
       "      <td>10.491</td>\n",
       "      <td>2.510147</td>\n",
       "      <td>44.0217</td>\n",
       "      <td>3.710090</td>\n",
       "      <td>20.46850</td>\n",
       "      <td>396.648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>72</td>\n",
       "      <td>29.136316</td>\n",
       "      <td>83</td>\n",
       "      <td>10.949</td>\n",
       "      <td>2.241625</td>\n",
       "      <td>26.8081</td>\n",
       "      <td>2.784910</td>\n",
       "      <td>14.76966</td>\n",
       "      <td>232.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>57</td>\n",
       "      <td>34.838148</td>\n",
       "      <td>95</td>\n",
       "      <td>12.548</td>\n",
       "      <td>2.940415</td>\n",
       "      <td>33.1612</td>\n",
       "      <td>2.364950</td>\n",
       "      <td>9.95420</td>\n",
       "      <td>655.834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>73</td>\n",
       "      <td>37.109375</td>\n",
       "      <td>134</td>\n",
       "      <td>5.636</td>\n",
       "      <td>1.862886</td>\n",
       "      <td>41.4064</td>\n",
       "      <td>3.335665</td>\n",
       "      <td>6.89235</td>\n",
       "      <td>788.902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>45</td>\n",
       "      <td>29.384757</td>\n",
       "      <td>90</td>\n",
       "      <td>4.713</td>\n",
       "      <td>1.046286</td>\n",
       "      <td>23.8479</td>\n",
       "      <td>6.644245</td>\n",
       "      <td>15.55625</td>\n",
       "      <td>621.273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>46</td>\n",
       "      <td>33.180000</td>\n",
       "      <td>92</td>\n",
       "      <td>5.750</td>\n",
       "      <td>1.304867</td>\n",
       "      <td>18.6900</td>\n",
       "      <td>9.160000</td>\n",
       "      <td>8.89000</td>\n",
       "      <td>209.190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>68</td>\n",
       "      <td>35.560000</td>\n",
       "      <td>131</td>\n",
       "      <td>8.150</td>\n",
       "      <td>2.633537</td>\n",
       "      <td>17.8700</td>\n",
       "      <td>11.900000</td>\n",
       "      <td>4.19000</td>\n",
       "      <td>198.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>75</td>\n",
       "      <td>30.480000</td>\n",
       "      <td>152</td>\n",
       "      <td>7.010</td>\n",
       "      <td>2.628283</td>\n",
       "      <td>50.5300</td>\n",
       "      <td>10.060000</td>\n",
       "      <td>11.73000</td>\n",
       "      <td>99.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>54</td>\n",
       "      <td>36.050000</td>\n",
       "      <td>119</td>\n",
       "      <td>11.910</td>\n",
       "      <td>3.495982</td>\n",
       "      <td>89.2700</td>\n",
       "      <td>8.010000</td>\n",
       "      <td>5.06000</td>\n",
       "      <td>218.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>45</td>\n",
       "      <td>26.850000</td>\n",
       "      <td>92</td>\n",
       "      <td>3.330</td>\n",
       "      <td>0.755688</td>\n",
       "      <td>54.6800</td>\n",
       "      <td>12.100000</td>\n",
       "      <td>10.96000</td>\n",
       "      <td>268.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>62</td>\n",
       "      <td>26.840000</td>\n",
       "      <td>100</td>\n",
       "      <td>4.530</td>\n",
       "      <td>1.117400</td>\n",
       "      <td>12.4500</td>\n",
       "      <td>21.420000</td>\n",
       "      <td>7.32000</td>\n",
       "      <td>330.160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>65</td>\n",
       "      <td>32.050000</td>\n",
       "      <td>97</td>\n",
       "      <td>5.730</td>\n",
       "      <td>1.370998</td>\n",
       "      <td>61.4800</td>\n",
       "      <td>22.540000</td>\n",
       "      <td>10.33000</td>\n",
       "      <td>314.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>72</td>\n",
       "      <td>25.590000</td>\n",
       "      <td>82</td>\n",
       "      <td>2.820</td>\n",
       "      <td>0.570392</td>\n",
       "      <td>24.9600</td>\n",
       "      <td>33.750000</td>\n",
       "      <td>3.27000</td>\n",
       "      <td>392.460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>86</td>\n",
       "      <td>27.180000</td>\n",
       "      <td>138</td>\n",
       "      <td>19.910</td>\n",
       "      <td>6.777364</td>\n",
       "      <td>90.2800</td>\n",
       "      <td>14.110000</td>\n",
       "      <td>4.35000</td>\n",
       "      <td>90.090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age        BMI  Glucose  Insulin       HOMA   Leptin  Adiponectin  \\\n",
       "0     48  23.500000       70    2.707   0.467409   8.8071     9.702400   \n",
       "1     83  20.690495       92    3.115   0.706897   8.8438     5.429285   \n",
       "2     82  23.124670       91    4.498   1.009651  17.9393    22.432040   \n",
       "3     68  21.367521       77    3.226   0.612725   9.8827     7.169560   \n",
       "4     86  21.111111       92    3.549   0.805386   6.6994     4.819240   \n",
       "5     49  22.854458       92    3.226   0.732087   6.8317    13.679750   \n",
       "6     89  22.700000       77    4.690   0.890787   6.9640     5.589865   \n",
       "7     76  23.800000      118    6.470   1.883201   4.3110    13.251320   \n",
       "8     73  22.000000       97    3.350   0.801543   4.4700    10.358725   \n",
       "9     75  23.000000       83    4.952   1.013839  17.1270    11.578990   \n",
       "10    34  21.470000       78    3.469   0.667436  14.5700    13.110000   \n",
       "11    29  23.010000       82    5.663   1.145436  35.5900    26.720000   \n",
       "12    25  22.860000       82    4.090   0.827271  20.4500    23.670000   \n",
       "13    24  18.670000       88    6.107   1.330000   8.8800    36.060000   \n",
       "14    38  23.340000       75    5.782   1.069670  15.2600    17.950000   \n",
       "15    44  20.760000       86    7.553   1.600000  14.0900    20.320000   \n",
       "16    47  22.030000       84    2.869   0.590000  26.6500    38.040000   \n",
       "17    61  32.038959       85   18.077   3.790144  30.7729     7.780255   \n",
       "18    64  34.529723       95    4.427   1.037394  21.2117     5.462620   \n",
       "19    32  36.512637       87   14.026   3.009980  49.3727     5.100000   \n",
       "20    36  28.576676       86    4.345   0.921719  15.1248     8.600000   \n",
       "21    34  31.975015       87    4.530   0.972138  28.7502     7.642760   \n",
       "22    29  32.270788       84    5.810   1.203832  45.6196     6.209635   \n",
       "23    35  30.276817       84    4.376   0.906707  39.2134     9.048185   \n",
       "24    54  30.483158       90    5.537   1.229214  12.3310     9.731380   \n",
       "25    45  37.035608       83    6.760   1.383997  39.9802     4.617125   \n",
       "26    50  38.578759      106    6.703   1.752611  46.6401     4.667645   \n",
       "27    66  31.446541       90    9.245   2.052390  45.9624    10.355260   \n",
       "28    35  35.250761       90    6.817   1.513374  50.6094     6.966895   \n",
       "29    36  34.174890       80    6.590   1.300427  10.2809     5.065915   \n",
       "..   ...        ...      ...      ...        ...      ...          ...   \n",
       "86    48  28.125000       90    2.540   0.563880  15.5325    10.222310   \n",
       "87    85  27.688778      196   51.814  25.050342  70.8824     7.901685   \n",
       "88    48  31.250000      199   12.162   5.969920  18.1314     4.104105   \n",
       "89    58  29.154519      139   16.582   5.685415  22.8884    10.262660   \n",
       "90    40  30.836531      128   41.894  13.227332  31.0385     6.160995   \n",
       "91    82  31.217482      100   18.077   4.458993  31.6453     9.923650   \n",
       "92    52  30.801249       87   30.212   6.483495  29.2739     6.268540   \n",
       "93    49  32.461911      134   24.887   8.225983  42.3914    10.793940   \n",
       "94    60  31.231410      131   30.130   9.736007  37.8430     8.404430   \n",
       "95    49  29.777778       70    8.396   1.449709  51.3387    10.731740   \n",
       "96    44  27.887617       99    9.208   2.248594  12.6757     5.478170   \n",
       "97    40  27.636054      103    2.432   0.617890  14.3224     6.783870   \n",
       "98    71  27.915519      104   18.200   4.668907  53.4997     1.656020   \n",
       "99    69  28.444444      108    8.808   2.346451  14.7485     5.288025   \n",
       "100   74  28.650138       88    3.012   0.653805  31.1233     7.652220   \n",
       "101   66  26.562500       89    6.524   1.432235  14.9084     8.429960   \n",
       "102   65  30.915577       97   10.491   2.510147  44.0217     3.710090   \n",
       "103   72  29.136316       83   10.949   2.241625  26.8081     2.784910   \n",
       "104   57  34.838148       95   12.548   2.940415  33.1612     2.364950   \n",
       "105   73  37.109375      134    5.636   1.862886  41.4064     3.335665   \n",
       "106   45  29.384757       90    4.713   1.046286  23.8479     6.644245   \n",
       "107   46  33.180000       92    5.750   1.304867  18.6900     9.160000   \n",
       "108   68  35.560000      131    8.150   2.633537  17.8700    11.900000   \n",
       "109   75  30.480000      152    7.010   2.628283  50.5300    10.060000   \n",
       "110   54  36.050000      119   11.910   3.495982  89.2700     8.010000   \n",
       "111   45  26.850000       92    3.330   0.755688  54.6800    12.100000   \n",
       "112   62  26.840000      100    4.530   1.117400  12.4500    21.420000   \n",
       "113   65  32.050000       97    5.730   1.370998  61.4800    22.540000   \n",
       "114   72  25.590000       82    2.820   0.570392  24.9600    33.750000   \n",
       "115   86  27.180000      138   19.910   6.777364  90.2800    14.110000   \n",
       "\n",
       "     Resistin     MCP.1  \n",
       "0     7.99585   417.114  \n",
       "1     4.06405   468.786  \n",
       "2     9.27715   554.697  \n",
       "3    12.76600   928.220  \n",
       "4    10.57635   773.920  \n",
       "5    10.31760   530.410  \n",
       "6    12.93610  1256.083  \n",
       "7     5.10420   280.694  \n",
       "8     6.28445   136.855  \n",
       "9     7.09130   318.302  \n",
       "10    6.92000   354.600  \n",
       "11    4.58000   174.800  \n",
       "12    5.14000   313.730  \n",
       "13    6.85000   632.220  \n",
       "14    9.35000   165.020  \n",
       "15    7.64000    63.610  \n",
       "16    3.32000   191.720  \n",
       "17   13.68392   444.395  \n",
       "18    6.70188   252.449  \n",
       "19   17.10223   588.460  \n",
       "20    9.15390   534.224  \n",
       "21    5.62592   572.783  \n",
       "22   24.60330   904.981  \n",
       "23   16.43706   733.797  \n",
       "24   10.19299  1227.910  \n",
       "25    8.70448   586.173  \n",
       "26   11.78388   887.160  \n",
       "27   23.38190  1102.110  \n",
       "28   22.03703   667.928  \n",
       "29   15.72187   581.313  \n",
       "..        ...       ...  \n",
       "86   16.11032  1698.440  \n",
       "87   55.21530  1078.359  \n",
       "88   53.63080  1698.440  \n",
       "89   13.97399   923.886  \n",
       "90   17.55503   638.261  \n",
       "91   19.94687   994.316  \n",
       "92   24.24591   764.667  \n",
       "93    5.76800   656.393  \n",
       "94   11.50005   396.021  \n",
       "95   20.76801   602.486  \n",
       "96   23.03306   407.206  \n",
       "97   26.01360   293.123  \n",
       "98   49.24184   256.001  \n",
       "99   16.48508   353.568  \n",
       "100  18.35574   572.401  \n",
       "101  14.91922   269.487  \n",
       "102  20.46850   396.648  \n",
       "103  14.76966   232.018  \n",
       "104   9.95420   655.834  \n",
       "105   6.89235   788.902  \n",
       "106  15.55625   621.273  \n",
       "107   8.89000   209.190  \n",
       "108   4.19000   198.400  \n",
       "109  11.73000    99.450  \n",
       "110   5.06000   218.280  \n",
       "111  10.96000   268.230  \n",
       "112   7.32000   330.160  \n",
       "113  10.33000   314.050  \n",
       "114   3.27000   392.460  \n",
       "115   4.35000    90.090  \n",
       "\n",
       "[116 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92, 9)\n",
      "(92, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_trainset.shape)\n",
    "print(y_trainset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 9)\n",
      "(24, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_testset.shape)\n",
    "print(y_testset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/ipykernel/__main__.py:5: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n"
     ]
    }
   ],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_trainset = sc.fit_transform(X_trainset)\n",
    "X_testset = sc.transform(X_testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying LDA\n",
    "#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "#lda = LDA(n_components = 8)\n",
    "#X_trainset = lda.fit_transform(X_trainset, y_trainset)\n",
    "#X_testset = lda.transform(X_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Kernel PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "kpca = KernelPCA(n_components = 7, kernel = 'rbf')\n",
    "X_trainset = kpca.fit_transform(X_trainset)\n",
    "X_testset = kpca.transform(X_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.13220414e-01,  2.49675750e-01, -1.64298302e-01,\n",
       "        -9.57085718e-02,  1.91128581e-02, -2.63561351e-02,\n",
       "         1.64476880e-02],\n",
       "       [-1.00862328e-01,  1.66526193e-01,  1.11319271e-01,\n",
       "         5.05869585e-02, -8.59500320e-02,  1.90351007e-03,\n",
       "        -3.00047025e-02],\n",
       "       [ 2.30546206e-01, -1.16852518e-01, -1.38096256e-01,\n",
       "         1.38911995e-01, -7.74033376e-03, -9.50534455e-03,\n",
       "        -3.07593535e-02],\n",
       "       [ 2.25419462e-01, -1.89029993e-01,  1.13381952e-01,\n",
       "         2.56258323e-02,  1.56807929e-01,  2.84108619e-02,\n",
       "        -1.50815485e-02],\n",
       "       [ 2.07882215e-01,  5.57590726e-03,  1.79012386e-01,\n",
       "        -3.20247922e-02, -4.86101475e-02,  4.77703498e-02,\n",
       "         3.57196102e-03],\n",
       "       [-1.22297262e-01,  1.47420815e-01,  1.23791271e-01,\n",
       "         6.32604592e-02,  2.15620041e-02,  1.94014352e-03,\n",
       "        -3.61120678e-02],\n",
       "       [-2.30582521e-01,  6.34903676e-02,  3.34180902e-02,\n",
       "         1.31635533e-01, -9.78964417e-02,  5.70844592e-03,\n",
       "         2.46354245e-03],\n",
       "       [-1.62792447e-01,  1.58672331e-01,  1.35844651e-01,\n",
       "         8.54026507e-02, -4.40716024e-02,  4.28914306e-03,\n",
       "        -3.19492979e-02],\n",
       "       [-3.49017663e-02, -8.93972663e-02, -1.97399220e-01,\n",
       "        -1.82527118e-01, -2.26494524e-03, -8.09577810e-04,\n",
       "        -2.45702469e-02],\n",
       "       [ 1.75514774e-01,  5.27124291e-03, -1.26901995e-01,\n",
       "         2.23812720e-01, -1.69757098e-02, -1.66193454e-02,\n",
       "        -1.22057818e-02],\n",
       "       [-2.98988638e-01,  2.22575019e-02,  7.74223418e-02,\n",
       "         3.70195066e-02,  2.39412204e-03,  1.98198567e-02,\n",
       "         8.45108759e-03],\n",
       "       [ 9.88641442e-02,  1.29217336e-01,  1.69876219e-01,\n",
       "        -1.60902019e-02,  1.81198582e-01,  1.59746384e-02,\n",
       "        -2.53780866e-02],\n",
       "       [-2.11073122e-01,  1.14734265e-01,  1.73793846e-01,\n",
       "         5.20329131e-02, -1.67746609e-01,  1.00997490e-02,\n",
       "        -1.52632635e-02],\n",
       "       [-2.37157638e-01,  2.68590219e-02, -2.68246377e-01,\n",
       "         7.91080799e-02,  6.13654278e-02,  4.54448723e-02,\n",
       "         1.78026966e-02],\n",
       "       [ 3.64490650e-02, -1.74051945e-01,  3.83975873e-02,\n",
       "        -1.67425894e-01,  8.48974134e-02, -1.14303104e-02,\n",
       "        -3.17187346e-02],\n",
       "       [-1.25209474e-01,  1.02690720e-01, -2.55779463e-02,\n",
       "         2.11912175e-01, -8.72813274e-04, -1.11855511e-02,\n",
       "        -1.15145581e-02],\n",
       "       [-8.91873903e-02, -6.81198638e-03, -9.03898376e-02,\n",
       "         2.35747820e-01,  1.71780098e-01, -2.50664180e-02,\n",
       "        -1.69110090e-04],\n",
       "       [ 1.82035466e-01,  9.55360781e-02,  1.57962678e-01,\n",
       "        -1.62838982e-01,  4.61388890e-02,  3.48861929e-02,\n",
       "        -8.82810613e-03],\n",
       "       [ 2.17196297e-01,  6.12101827e-02, -1.72695932e-01,\n",
       "         1.20974643e-02, -1.92297142e-01, -1.00461826e-02,\n",
       "        -1.29278641e-02],\n",
       "       [ 2.51578905e-01,  9.62235954e-02, -1.77917449e-01,\n",
       "         1.23537812e-03, -1.35706732e-01, -7.37573753e-03,\n",
       "        -8.94744546e-03],\n",
       "       [-6.38929879e-02,  2.88694652e-01,  1.17107905e-01,\n",
       "        -6.25010053e-02,  1.17908690e-01, -6.79705160e-03,\n",
       "        -3.95752445e-02],\n",
       "       [-6.43145412e-02,  2.82610265e-01,  9.33384815e-02,\n",
       "        -2.13512323e-02,  7.50160370e-02, -6.94919842e-03,\n",
       "        -3.77601581e-02],\n",
       "       [-3.00885329e-01, -5.34237862e-02, -1.08940265e-01,\n",
       "         1.48230806e-02,  5.73223911e-02,  3.21762011e-02,\n",
       "         3.99022057e-02],\n",
       "       [-1.21767133e-01,  1.04637775e-01,  2.90037486e-02,\n",
       "         2.26526610e-01,  1.25037135e-01, -1.51101440e-02,\n",
       "        -1.24363346e-02]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using KNN for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/ipykernel/__main__.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    }
   ],
   "source": [
    "# Fitting K-NN to the Training set\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "classifier.fit(X_trainset, y_trainset)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_testset)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "knn_cm = confusion_matrix(y_testset, y_pred)\n",
    "acuracy  = cross_val_score(estimator = classifier, X=X_trainset, y=y_trainset, cv = 10)\n",
    "knn_acuracy = acuracy.mean()\n",
    "knn_precision_score = precision_score(y_pred, y_testset)\n",
    "knn_recall_score = recall_score(y_pred, y_testset)\n",
    "knn_f1_score = f1_score(y_pred, y_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  5]\n",
      " [ 2 11]]\n",
      "0.5454545454545454\n",
      "0.75\n",
      "0.631578947368421\n"
     ]
    }
   ],
   "source": [
    "print(knn_cm)\n",
    "print(knn_precision_score)\n",
    "print(knn_recall_score)\n",
    "print(knn_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SVM for classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=0,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier.fit(X_trainset, y_trainset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_testset)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "svm_cm = confusion_matrix(y_testset, y_pred)\n",
    "acuracy  = cross_val_score(estimator = classifier, X=X_trainset, y=y_trainset, cv = 10)\n",
    "svm_acuracy = acuracy.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  6],\n",
       "       [ 1, 12]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Kernel SVM for classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=0,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier.fit(X_trainset, y_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_testset)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "ksvm_cm = confusion_matrix(y_testset, y_pred)\n",
    "acuracy  = cross_val_score(estimator = classifier, X=X_trainset, y=y_trainset, cv = 10)\n",
    "ksvm_acuracy = acuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 11],\n",
       "       [ 0, 13]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ksvm_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Naive Bayes for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_trainset, y_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_testset)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "naive_base_cm = confusion_matrix(y_testset, y_pred)\n",
    "acuracy  = cross_val_score(estimator = classifier, X=X_trainset, y=y_trainset, cv = 10)\n",
    "naive_bayes_acuracy = acuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  4],\n",
       "       [ 2, 11]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_base_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Decission Tree classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Decision Tree Classification to the Training set\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_trainset, y_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_testset)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "tre_cm = confusion_matrix(y_testset, y_pred)\n",
    "acuracy  = cross_val_score(estimator = classifier, X=X_trainset, y=y_trainset, cv = 10)\n",
    "tree_acuracy = acuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 6],\n",
       "       [4, 9]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tre_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/ipykernel/__main__.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Random Forest Classification to the Training set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_trainset, y_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_testset)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "random_tree_cm = confusion_matrix(y_testset, y_pred)\n",
    "acuracy  = cross_val_score(estimator = classifier, X=X_trainset, y=y_trainset, cv = 10)\n",
    "random_tree_acuracy = acuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  3],\n",
       "       [ 1, 12]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tree_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X_trainset, y_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_testset)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "logistic_cm = confusion_matrix(y_testset, y_pred)\n",
    "acuracy  = cross_val_score(estimator = classifier, X=X_trainset, y=y_trainset, cv = 10)\n",
    "logistic_acuracy = acuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  9],\n",
       "       [ 1, 12]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "classifier = XGBClassifier()\n",
    "classifier.fit(X_trainset, y_trainset)\n",
    "y_pred = classifier.predict(X_testset)\n",
    "xgboost_cm = confusion_matrix(y_testset, y_pred)\n",
    "acuracy  = cross_val_score(estimator = classifier, X=X_trainset, y=y_trainset, cv = 10)\n",
    "xgboost_acuracy = acuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  4],\n",
       "       [ 2, 11]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output all algorithms acuracy means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn_acuracy: 0.7686868686868686\n",
      "svm_acuracy: 0.6323232323232324\n",
      "ksvm_acuracy: 0.5545454545454545\n",
      "naive_bayes_acuracy: 0.6727272727272726\n",
      "tree_acuracy: 0.6434343434343435\n",
      "random_tree_acuracy: 0.7282828282828283\n",
      "logistic_acuracy: 0.6414141414141414\n",
      "xgboost_cm: 0.687878787878788\n"
     ]
    }
   ],
   "source": [
    "print(\"knn_acuracy: \" + str(knn_acuracy) )\n",
    "print(\"svm_acuracy: \" + str(svm_acuracy) )\n",
    "print(\"ksvm_acuracy: \" + str(ksvm_acuracy) )\n",
    "print(\"naive_bayes_acuracy: \" + str(naive_bayes_acuracy) )\n",
    "print(\"tree_acuracy: \" + str(tree_acuracy) )\n",
    "print(\"random_tree_acuracy: \" + str(random_tree_acuracy) )\n",
    "print(\"logistic_acuracy: \" + str(logistic_acuracy) )\n",
    "print(\"xgboost_cm: \" + str(xgboost_acuracy) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcdefaults()\n",
    "fig, ax = plt.subplots()\n",
    "classificators = (\"KNN\", \"SVM\", \"KSVM\", \"Naive Bayes\", \"Dessicion Tree\", \"Random Forest\", \"Logistic Regression\", \"XGBoost\")\n",
    "arr_acuracy = [knn_acuracy * 100, svm_acuracy * 100, ksvm_acuracy * 100, naive_bayes_acuracy * 100, tree_acuracy * 100, random_tree_acuracy * 100, logistic_acuracy * 100, xgboost_acuracy * 100];\n",
    "y_pos = np.arange(len(classificators))\n",
    "ax.set_xlim([0,100])\n",
    "ax.barh(y_pos, arr_acuracy,  align='center')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(classificators)\n",
    "ax.invert_yaxis()  # labels read top-to-bottom\n",
    "ax.set_xlabel('Accuracy')\n",
    "ax.set_title('Classification models accuracy given Breast cancer dataset')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
